{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e88efbd5-2215-4edc-a60e-8dfefd25aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/code_data.csv\"\n",
    "save_dir = \"model/voltscript/gpt2-large-hf\"\n",
    "base_model = \"openai-community/gpt2-large\"\n",
    "tok_dir = \"tokenizer/voltscript-bpe\"\n",
    "n_examples = 60\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc1c0a88-f0bf-4f5e-a293-dd31f12ac0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset prep\n",
    "\"\"\"\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class ConstantLengthDataset(IterableDataset):\n",
    "    def __init__(\n",
    "        self, tokenizer, dataset, infinite=False, seq_length=512, num_of_sequences=512, chars_per_token=3.6\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concat_token_id = tokenizer.bos_token_id\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.input_characters = seq_length * chars_per_token * num_of_sequences\n",
    "        self.epoch = 0\n",
    "        self.infinite = infinite\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        more_examples = True\n",
    "        while more_examples:\n",
    "            buffer, buffer_len = [], 0\n",
    "            while True:\n",
    "                if buffer_len >= self.input_characters:\n",
    "                    break\n",
    "                try:\n",
    "                    buffer.append(next(iterator)[\"code\"])\n",
    "                    buffer_len += len(buffer[-1])\n",
    "                except StopIteration:\n",
    "                    if self.infinite:\n",
    "                        iterator = iter(self.dataset)\n",
    "                        self.epoch += 1\n",
    "                        print(f\"Dataset epoch: {self.epoch}\")\n",
    "                    else:\n",
    "                        more_examples = False\n",
    "                        break\n",
    "                except TypeError:\n",
    "                    pass\n",
    "            tokenized_inputs = self.tokenizer(buffer, truncation=True)[\"input_ids\"]\n",
    "            all_token_ids = []\n",
    "            for tokenized_input in tokenized_inputs:\n",
    "                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
    "            for i in range(0, len(all_token_ids), self.seq_length):\n",
    "                input_ids = all_token_ids[i : i + self.seq_length]\n",
    "                if len(input_ids) == self.seq_length:\n",
    "                    yield torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701cfed3-adf3-4be0-96ff-62c039eb5113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, HfArgumentParser\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bcytes_to_unicode\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, HfArgumentParser\n",
    "from accelerate import Accelerator, DistributedType\n",
    "from transformers import HfArgumentParser, set_seed\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "\n",
    "def batch_iterator(n_examples, batch_size=1):\n",
    "    \"\"\" \n",
    "    Iterator for Training\n",
    "    \"\"\"\n",
    "    for _ in tqdm(range(0, n_examples, batch_size)):\n",
    "        try:\n",
    "            yield [next(iter_dataset)[\"code\"] for _ in range(batch_size)]\n",
    "        except StopIteration:\n",
    "            return\n",
    "\n",
    "\n",
    "def build_tokenizer(data_dir):\n",
    "    \"\"\"\n",
    "    base tokenizer\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    base_vocab = list(bytes_to_unicode().values())\n",
    "    \n",
    "    # Load dataset\n",
    "    df = pd.read_csv(data_dir)\n",
    "    df = df.dropna()\n",
    "    dataset = Dataset.from_pandas(df[:-(int(len(df)*0.2))])\n",
    "    iter_dataset = iter(dataset)\n",
    "    print(iter_dataset)\n",
    "    \n",
    "    # Training and saving\n",
    "    new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(len(df[:-(int(len(df)*0.2))])),\n",
    "                                                      vocab_size=tokenizer.vocab_size,\n",
    "                                                      initial_alphabet=base_vocab\n",
    "                                                     )\n",
    "    new_tokenizer.save_pretrained(tok_dir, push_to_hub=False)\n",
    "\n",
    "\n",
    "def load_tokenizer():\n",
    "    \"\"\"\n",
    "    Load built tokenizer\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tok_dir)\n",
    "\n",
    "    # Config: \"scale_attn_by_layer_idx\" and \"reorder_and_upcast_attn\" are Mistral stability tweaks\n",
    "    config_kwargs = {\n",
    "        \"vocab_size\": len(tokenizer),\n",
    "        \"scale_attn_by_inverse_layer_idx\": True,\n",
    "        \"reorder_and_upcast_attn\": True,\n",
    "    }\n",
    "\n",
    "    # Load model config (GPT-2)\n",
    "    config = AutoConfig.from_pretrained(base_model, **config_kwargs)\n",
    "    \n",
    "    # Initialize new model with config\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "    \n",
    "    Save model to the hub\n",
    "    model.save_pretrained(save_dir, push_to_hub=False)\n",
    "\n",
    "\n",
    "def init_accelerator():\n",
    "    \"\"\"\n",
    "    initialize accelerator\n",
    "    \"\"\"\n",
    "    accelerator = Accelerator()\n",
    "    acc_state = {str(k): str(v) for k, v in accelerator.state.__dict__.items()}\n",
    "    \n",
    "    samples_per_step = accelerator.state.num_processes * 2\n",
    "    set_seed(seed)\n",
    "    return accelerator\n",
    "\n",
    "\n",
    "def create_dataloaders():\n",
    "    \"\"\"\n",
    "    create dataloaders\n",
    "    \"\"\"\n",
    "    train_data = Dataset.from_pandas(df[:-(int(len(df)*0.2))])\n",
    "    train_data = train_data.shuffle()\n",
    "    valid_data = Dataset.from_pandas(df[-(int(len(df)*0.2)):])\n",
    "    \n",
    "    train_dataset = ConstantLengthDataset(tokenizer, train_data, infinite=True, seq_length=512)\n",
    "    valid_dataset = ConstantLengthDataset(tokenizer, valid_data, infinite=False, seq_length=512)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=4)\n",
    "    eval_dataloader = DataLoader(valid_dataset, batch_size=4)\n",
    "    return train_dataloader, eval_dataloader\n",
    "\n",
    "train_dataloader, eval_dataloader = create_dataloaders()\n",
    "\n",
    "\n",
    "def load_base(save_dir, gradient_checkpointing):\n",
    "    \"\"\"\n",
    "    Load base model and tokenizer\n",
    "    \"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(save_dir)\n",
    "    if gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "\n",
    "    return model, tokenizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a474706c-8a03-4c56-beff-7c07e80be2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564284e-f0fc-44cf-a5be-5063853d118d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8409f69c-34a8-4e61-9f6b-4fefd01adb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "\n",
    "class ConstantLengthDataset(IterableDataset):\n",
    "    def __init__(\n",
    "        self, tokenizer, dataset, infinite=False, seq_length=512, num_of_sequences=512, chars_per_token=3.6\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concat_token_id = tokenizer.bos_token_id\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.input_characters = seq_length * chars_per_token * num_of_sequences\n",
    "        self.epoch = 0\n",
    "        self.infinite = infinite\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        more_examples = True\n",
    "        while more_examples:\n",
    "            buffer, buffer_len = [], 0\n",
    "            while True:\n",
    "                if buffer_len >= self.input_characters:\n",
    "                    break\n",
    "                try:\n",
    "                    buffer.append(next(iterator)[\"code\"])\n",
    "                    buffer_len += len(buffer[-1])\n",
    "                except StopIteration:\n",
    "                    if self.infinite:\n",
    "                        iterator = iter(self.dataset)\n",
    "                        self.epoch += 1\n",
    "                        print(f\"Dataset epoch: {self.epoch}\")\n",
    "                    else:\n",
    "                        more_examples = False\n",
    "                        break\n",
    "                except TypeError:\n",
    "                    pass\n",
    "            tokenized_inputs = self.tokenizer(buffer, truncation=True)[\"input_ids\"]\n",
    "            all_token_ids = []\n",
    "            for tokenized_input in tokenized_inputs:\n",
    "                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
    "            for i in range(0, len(all_token_ids), self.seq_length):\n",
    "                input_ids = all_token_ids[i : i + self.seq_length]\n",
    "                if len(input_ids) == self.seq_length:\n",
    "                    yield torch.tensor(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dc148c-c68c-4404-8f29-8f90e8cb85a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "def create_dataloaders():\n",
    "    train_data = Dataset.from_pandas(df[:-(int(len(df)*0.2))])\n",
    "    train_data = train_data.shuffle()\n",
    "    valid_data = Dataset.from_pandas(df[-(int(len(df)*0.2)):])\n",
    "    \n",
    "    train_dataset = ConstantLengthDataset(tokenizer, train_data, infinite=True, seq_length=512)\n",
    "    valid_dataset = ConstantLengthDataset(tokenizer, valid_data, infinite=False, seq_length=512)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=4)\n",
    "    eval_dataloader = DataLoader(valid_dataset, batch_size=4)\n",
    "    return train_dataloader, eval_dataloader\n",
    "\n",
    "train_dataloader, eval_dataloader = create_dataloaders()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a74a433-bc77-4b44-9928-cccecf8cc74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay): params_without_wd.append(p)\n",
    "        else: params_with_wd.append(p)\n",
    "    return [{\"params\": params_with_wd, \"weight_decay\": 0.1},\n",
    "            {\"params\": params_without_wd, \"weight_decay\": 0.0},]\n",
    "\n",
    "optimizer = AdamW(get_grouped_params(model), lr=0.005)\n",
    "lr_scheduler = get_scheduler(name='cosine', optimizer=optimizer,\n",
    "                             num_warmup_steps=100,\n",
    "                             num_training_steps=500,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a1a89c-e41f-4866-8ba1-ca6a99da3b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7340104b-9ad6-466b-a7b4-ad39380b049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss.repeat(2)\n",
    "        losses.append(accelerator.gather(loss))\n",
    "        if -1 > 0 and step >= -1:\n",
    "            break\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf70110-5a88-43d0-b76c-a32c78867798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "for step, batch in enumerate(train_dataloader, start=1):\n",
    "    loss = model(batch, labels=batch, use_cache=False).loss\n",
    "    loss = loss / 16\n",
    "    accelerator.backward(loss)\n",
    "    if step % 16 == 0:\n",
    "        accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        completed_steps += 1\n",
    "    if step % 100 == 0:\n",
    "        eval_loss, perplexity = evaluate()\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(model_dir, save_function=accelerator.save)\n",
    "        # if accelerator.is_main_process:\n",
    "        #     hf_repo.push_to_hub(commit_message=f\"step {step}\")\n",
    "        model.train()\n",
    "    if completed_steps >= 100:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
